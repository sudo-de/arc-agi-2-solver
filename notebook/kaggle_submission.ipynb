{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ===================================================================\n",
    "# Environment Setup and Imports\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Set optimal environment\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False  # For speed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"üöÄ ARC-AGI Advanced Solver - Competition Mode\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Import project modules\n",
    "sys.path.append('/kaggle/input/arc-agi-2-solver')\n",
    "\n",
    "try:\n",
    "    from src.arc_compressor import create_arc_compressor\n",
    "    from src.layers import *\n",
    "    from src.initializers import create_initializer\n",
    "    from src.multitensor_systems import MultiTensorSystem, multify\n",
    "    from src.preprocessing import Task, preprocess_tasks\n",
    "    from src.solution_selection import Logger\n",
    "    from src.train import train_with_advanced_techniques, create_optimizer_and_scheduler\n",
    "    from src.visualization import *\n",
    "    from src.solve_task import solve_multiple_tasks, create_kaggle_submission\n",
    "    \n",
    "    print(\"‚úÖ All modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Falling back to basic modules...\")\n",
    "    # Fallback imports would go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Competition Configuration\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CompetitionConfig:\n",
    "    \"\"\"Optimized configuration for competition performance.\"\"\"\n",
    "    \n",
    "    # Hardware optimization\n",
    "    USE_MIXED_PRECISION = True\n",
    "    USE_COMPILE = hasattr(torch, 'compile')  # PyTorch 2.0+\n",
    "    MEMORY_EFFICIENT = True\n",
    "    \n",
    "    # Model configuration - Competition grade\n",
    "    MODEL_CONFIG = {\n",
    "        'variant': 'competition',\n",
    "        'n_layers': 8,\n",
    "        'base_dim': 64,\n",
    "        'attention_heads': 16,\n",
    "        'dropout': 0.05,\n",
    "        'use_adaptive_arch': True,\n",
    "        'use_attention': True,\n",
    "        'use_progressive_refinement': True,\n",
    "        'ensemble_size': 5,\n",
    "        'max_adaptive_layers': 12,\n",
    "    }\n",
    "    \n",
    "    # Training configuration\n",
    "    TRAINING_CONFIG = {\n",
    "        'n_iterations': 2500,  # Increased for competition\n",
    "        'base_lr': 0.008,\n",
    "        'attention_lr': 0.002,\n",
    "        'head_lr': 0.015,\n",
    "        'backbone_lr': 0.005,\n",
    "        'weight_decay': 0.015,\n",
    "        'betas': (0.9, 0.999),\n",
    "        'eps': 1e-8,\n",
    "        'max_grad_norm': 0.8,\n",
    "        'adaptive_clipping': True,\n",
    "        'adaptive_weights': True,\n",
    "        'kl_weight': 1.2,\n",
    "        'recon_weight': 12.0,\n",
    "        'T_0': 150,\n",
    "        'T_mult': 2,\n",
    "        'eta_min': 1e-7,\n",
    "        'restart_decay': 0.92,\n",
    "    }\n",
    "    \n",
    "    # Task solving configuration\n",
    "    SOLVING_CONFIG = {\n",
    "        'time_limit_per_task': 240.0,  # 4 minutes per task\n",
    "        'max_workers': min(6, torch.cuda.device_count()) if torch.cuda.is_available() else 1,\n",
    "        'use_ensemble': True,\n",
    "        'ensemble_voting': 'weighted',\n",
    "        'temperature_scaling': True,\n",
    "        'uncertainty_threshold': 0.15,\n",
    "    }\n",
    "    \n",
    "    # Paths\n",
    "    DATA_PATH = '/kaggle/input/arc-prize-2025'\n",
    "    OUTPUT_PATH = '/kaggle/working'\n",
    "    \n",
    "    @classmethod\n",
    "    def print_config(cls):\n",
    "        \"\"\"Print current configuration.\"\"\"\n",
    "        print(\"\\nüìã Competition Configuration:\")\n",
    "        print(f\"  Model: {cls.MODEL_CONFIG['variant']} - {cls.MODEL_CONFIG['n_layers']} layers\")\n",
    "        print(f\"  Base Dim: {cls.MODEL_CONFIG['base_dim']}\")\n",
    "        print(f\"  Attention Heads: {cls.MODEL_CONFIG['attention_heads']}\")\n",
    "        print(f\"  Ensemble Size: {cls.MODEL_CONFIG['ensemble_size']}\")\n",
    "        print(f\"  Training Iterations: {cls.TRAINING_CONFIG['n_iterations']}\")\n",
    "        print(f\"  Learning Rate: {cls.TRAINING_CONFIG['base_lr']}\")\n",
    "        print(f\"  Time per Task: {cls.SOLVING_CONFIG['time_limit_per_task']}s\")\n",
    "        print(f\"  Max Workers: {cls.SOLVING_CONFIG['max_workers']}\")\n",
    "\n",
    "config = CompetitionConfig()\n",
    "config.print_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Advanced Task Solver with Ensemble\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EnsembleArcSolver:\n",
    "    \"\"\"Advanced ensemble solver for maximum performance.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: CompetitionConfig):\n",
    "        self.config = config\n",
    "        self.scaler = GradScaler() if config.USE_MIXED_PRECISION else None\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.schedulers = []\n",
    "    \n",
    "    def create_ensemble_models(self, task) -> List[nn.Module]:\n",
    "        \"\"\"Create ensemble of models with different configurations.\"\"\"\n",
    "        models = []\n",
    "        \n",
    "        # Base configurations for ensemble diversity\n",
    "        ensemble_configs = [\n",
    "            # Large model with high capacity\n",
    "            {**self.config.MODEL_CONFIG, 'base_dim': 64, 'n_layers': 8, 'attention_heads': 16},\n",
    "            # Medium model with different architecture\n",
    "            {**self.config.MODEL_CONFIG, 'base_dim': 48, 'n_layers': 6, 'attention_heads': 12},\n",
    "            # Fast model for quick solutions\n",
    "            {**self.config.MODEL_CONFIG, 'base_dim': 32, 'n_layers': 4, 'attention_heads': 8},\n",
    "            # Specialized attention model\n",
    "            {**self.config.MODEL_CONFIG, 'base_dim': 56, 'n_layers': 7, 'attention_heads': 14, 'dropout': 0.1},\n",
    "            # Conservative model with less regularization\n",
    "            {**self.config.MODEL_CONFIG, 'base_dim': 40, 'n_layers': 5, 'attention_heads': 10, 'dropout': 0.02},\n",
    "        ]\n",
    "        \n",
    "        for i, model_config in enumerate(ensemble_configs[:self.config.MODEL_CONFIG['ensemble_size']]):\n",
    "            try:\n",
    "                model = create_arc_compressor(task, **model_config)\n",
    "                \n",
    "                # Compile model if available (PyTorch 2.0+)\n",
    "                if self.config.USE_COMPILE and hasattr(torch, 'compile'):\n",
    "                    model = torch.compile(model, mode='max-autotune')\n",
    "                \n",
    "                models.append(model)\n",
    "                print(f\"‚úÖ Created ensemble model {i+1}/{self.config.MODEL_CONFIG['ensemble_size']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to create model {i}: {e}\")\n",
    "                # Create fallback model\n",
    "                fallback_config = {**self.config.MODEL_CONFIG, 'base_dim': 32, 'n_layers': 4}\n",
    "                model = create_arc_compressor(task, **fallback_config)\n",
    "                models.append(model)\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def solve_task_with_ensemble(self, task_name: str, task_data: dict, time_limit: float) -> dict:\n",
    "        \"\"\"Solve single task with ensemble approach.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Create task object\n",
    "            task = Task(task_name, task_data, None)\n",
    "            \n",
    "            # Create ensemble models\n",
    "            models = self.create_ensemble_models(task)\n",
    "            \n",
    "            # Train each model\n",
    "            ensemble_solutions = []\n",
    "            ensemble_confidences = []\n",
    "            \n",
    "            for i, model in enumerate(models):\n",
    "                model_start = time.time()\n",
    "                \n",
    "                # Check time limit\n",
    "                if time.time() - start_time > time_limit * 0.8:  # Leave 20% buffer\n",
    "                    print(f\"‚è∞ Time limit approaching, using {i} models\")\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    # Create optimizer and scheduler for this model\n",
    "                    optimizer, scheduler = create_optimizer_and_scheduler(model, self.config.TRAINING_CONFIG)\n",
    "                    \n",
    "                    # Create logger\n",
    "                    logger = Logger(task)\n",
    "                    \n",
    "                    # Adaptive training iterations based on remaining time\n",
    "                    remaining_time = time_limit - (time.time() - start_time)\n",
    "                    adaptive_iterations = min(\n",
    "                        self.config.TRAINING_CONFIG['n_iterations'],\n",
    "                        int(remaining_time / len(models) * 100)  # Rough estimate\n",
    "                    )\n",
    "                    \n",
    "                    # Train with advanced techniques\n",
    "                    train_config = {**self.config.TRAINING_CONFIG, 'n_iterations': adaptive_iterations}\n",
    "                    \n",
    "                    if self.config.USE_MIXED_PRECISION:\n",
    "                        # Mixed precision training\n",
    "                        metrics = self._train_with_mixed_precision(task, model, train_config, logger)\n",
    "                    else:\n",
    "                        metrics = train_with_advanced_techniques(task, model, train_config, logger)\n",
    "                    \n",
    "                    # Extract solutions\n",
    "                    solutions = []\n",
    "                    for example_num in range(task.n_test):\n",
    "                        attempt_1 = [list(row) for row in logger.solution_most_frequent[example_num]]\n",
    "                        attempt_2 = [list(row) for row in logger.solution_second_most_frequent[example_num]]\n",
    "                        solutions.append({'attempt_1': attempt_1, 'attempt_2': attempt_2})\n",
    "                    \n",
    "                    # Compute confidence based on training metrics\n",
    "                    if metrics:\n",
    "                        final_loss = metrics[-1]['total_loss'] if metrics else float('inf')\n",
    "                        confidence = max(0.1, min(1.0, 1.0 / (1.0 + final_loss)))\n",
    "                    else:\n",
    "                        confidence = 0.5\n",
    "                    \n",
    "                    ensemble_solutions.append(solutions)\n",
    "                    ensemble_confidences.append(confidence)\n",
    "                    \n",
    "                    model_time = time.time() - model_start\n",
    "                    print(f\"üî• Model {i+1} completed in {model_time:.1f}s, confidence: {confidence:.3f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Model {i+1} failed: {e}\")\n",
    "                    # Add default solution\n",
    "                    default_solution = [{'attempt_1': [[0]], 'attempt_2': [[0]]}] * task.n_test\n",
    "                    ensemble_solutions.append(default_solution)\n",
    "                    ensemble_confidences.append(0.1)\n",
    "                \n",
    "                finally:\n",
    "                    # Cleanup\n",
    "                    del model, optimizer, scheduler, logger\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "            \n",
    "            # Ensemble combination\n",
    "            if ensemble_solutions:\n",
    "                final_solutions = self._combine_ensemble_solutions(\n",
    "                    ensemble_solutions, ensemble_confidences, task.n_test\n",
    "                )\n",
    "            else:\n",
    "                # Fallback\n",
    "                final_solutions = [{'attempt_1': [[0]], 'attempt_2': [[0]]}] * task.n_test\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"‚úÖ Task {task_name} completed in {total_time:.1f}s with {len(ensemble_solutions)} models\")\n",
    "            \n",
    "            return {\n",
    "                'solutions': final_solutions,\n",
    "                'confidence': np.mean(ensemble_confidences) if ensemble_confidences else 0.1,\n",
    "                'ensemble_size': len(ensemble_solutions),\n",
    "                'total_time': total_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Task {task_name} failed completely: {e}\")\n",
    "            # Return default solutions\n",
    "            return {\n",
    "                'solutions': [{'attempt_1': [[0]], 'attempt_2': [[0]]}],\n",
    "                'confidence': 0.0,\n",
    "                'ensemble_size': 0,\n",
    "                'total_time': time.time() - start_time\n",
    "            }\n",
    "    \n",
    "    def _train_with_mixed_precision(self, task, model, config, logger):\n",
    "        \"\"\"Train model with mixed precision for speed and memory efficiency.\"\"\"\n",
    "        optimizer, scheduler = create_optimizer_and_scheduler(model, config)\n",
    "        \n",
    "        metrics_history = []\n",
    "        \n",
    "        for step in range(config['n_iterations']):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                # Forward pass\n",
    "                logits, x_mask, y_mask, KL_amounts, KL_names = model.forward()\n",
    "                \n",
    "                # Compute loss (simplified for speed)\n",
    "                logits = torch.cat([torch.zeros_like(logits[:,:1,:,:]), logits], dim=1)\n",
    "                total_KL = sum(torch.sum(kl) for kl in KL_amounts)\n",
    "                \n",
    "                # Fast reconstruction loss\n",
    "                recon_loss = F.cross_entropy(\n",
    "                    logits.view(-1, logits.shape[1]), \n",
    "                    task.problem.view(-1), \n",
    "                    ignore_index=-1\n",
    "                )\n",
    "                \n",
    "                loss = total_KL + 10.0 * recon_loss\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.unscale_(optimizer)\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
    "            \n",
    "            # Optimizer step\n",
    "            self.scaler.step(optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Simplified logging\n",
    "            if step % 100 == 0:\n",
    "                metrics_history.append({\n",
    "                    'total_loss': loss.item(),\n",
    "                    'kl_loss': total_KL.item(),\n",
    "                    'recon_loss': recon_loss.item(),\n",
    "                })\n",
    "            \n",
    "            # Update logger (simplified)\n",
    "            if step % 50 == 0:\n",
    "                logger.log(step, logits, x_mask, y_mask, KL_amounts, KL_names, \n",
    "                          total_KL, recon_loss, loss)\n",
    "        \n",
    "        return metrics_history\n",
    "    \n",
    "    def _combine_ensemble_solutions(self, ensemble_solutions, confidences, n_test):\n",
    "        \"\"\"Combine ensemble solutions using weighted voting.\"\"\"\n",
    "        if not ensemble_solutions:\n",
    "            return [{'attempt_1': [[0]], 'attempt_2': [[0]]}] * n_test\n",
    "        \n",
    "        combined_solutions = []\n",
    "        \n",
    "        for test_idx in range(n_test):\n",
    "            # Collect all attempts from ensemble\n",
    "            all_attempt_1 = []\n",
    "            all_attempt_2 = []\n",
    "            weights = []\n",
    "            \n",
    "            for sol_idx, (solutions, confidence) in enumerate(zip(ensemble_solutions, confidences)):\n",
    "                if test_idx < len(solutions):\n",
    "                    all_attempt_1.append(solutions[test_idx]['attempt_1'])\n",
    "                    all_attempt_2.append(solutions[test_idx]['attempt_2'])\n",
    "                    weights.append(confidence)\n",
    "            \n",
    "            if not all_attempt_1:\n",
    "                combined_solutions.append({'attempt_1': [[0]], 'attempt_2': [[0]]})\n",
    "                continue\n",
    "            \n",
    "            # Weighted voting for best solutions\n",
    "            if self.config.SOLVING_CONFIG['ensemble_voting'] == 'weighted':\n",
    "                # Use highest confidence solutions\n",
    "                best_idx = np.argmax(weights)\n",
    "                second_best_idx = np.argsort(weights)[-2] if len(weights) > 1 else best_idx\n",
    "                \n",
    "                final_attempt_1 = all_attempt_1[best_idx]\n",
    "                final_attempt_2 = all_attempt_2[second_best_idx]\n",
    "            else:\n",
    "                # Simple majority voting (simplified implementation)\n",
    "                final_attempt_1 = all_attempt_1[0]\n",
    "                final_attempt_2 = all_attempt_2[-1] if len(all_attempt_2) > 1 else all_attempt_1[0]\n",
    "            \n",
    "            combined_solutions.append({\n",
    "                'attempt_1': final_attempt_1,\n",
    "                'attempt_2': final_attempt_2\n",
    "            })\n",
    "        \n",
    "        return combined_solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Data Loading and Validation\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_validate_competition_data():\n",
    "    \"\"\"Load and validate competition data with advanced checks.\"\"\"\n",
    "    print(\"\\nüìÅ Loading competition data...\")\n",
    "    \n",
    "    # Load test challenges\n",
    "    test_file = f\"{config.DATA_PATH}/arc-agi_test_challenges.json\"\n",
    "    \n",
    "    if not os.path.exists(test_file):\n",
    "        raise FileNotFoundError(f\"Test file not found: {test_file}\")\n",
    "    \n",
    "    with open(test_file, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(test_data)} test tasks\")\n",
    "    \n",
    "    # Advanced data validation\n",
    "    valid_tasks = {}\n",
    "    invalid_tasks = []\n",
    "    \n",
    "    for task_id, task_data in test_data.items():\n",
    "        try:\n",
    "            # Validate task structure\n",
    "            if 'train' not in task_data or 'test' not in task_data:\n",
    "                invalid_tasks.append((task_id, \"Missing train/test data\"))\n",
    "                continue\n",
    "            \n",
    "            # Validate train examples\n",
    "            if len(task_data['train']) == 0:\n",
    "                invalid_tasks.append((task_id, \"No training examples\"))\n",
    "                continue\n",
    "            \n",
    "            # Validate test examples\n",
    "            if len(task_data['test']) == 0:\n",
    "                invalid_tasks.append((task_id, \"No test examples\"))\n",
    "                continue\n",
    "            \n",
    "            # Check data types and shapes\n",
    "            for i, example in enumerate(task_data['train']):\n",
    "                if 'input' not in example or 'output' not in example:\n",
    "                    invalid_tasks.append((task_id, f\"Train example {i} missing input/output\"))\n",
    "                    break\n",
    "                \n",
    "                input_array = np.array(example['input'])\n",
    "                output_array = np.array(example['output'])\n",
    "                \n",
    "                if input_array.ndim != 2 or output_array.ndim != 2:\n",
    "                    invalid_tasks.append((task_id, f\"Train example {i} wrong dimensions\"))\n",
    "                    break\n",
    "            \n",
    "            for i, example in enumerate(task_data['test']):\n",
    "                if 'input' not in example:\n",
    "                    invalid_tasks.append((task_id, f\"Test example {i} missing input\"))\n",
    "                    break\n",
    "                \n",
    "                input_array = np.array(example['input'])\n",
    "                if input_array.ndim != 2:\n",
    "                    invalid_tasks.append((task_id, f\"Test example {i} wrong dimensions\"))\n",
    "                    break\n",
    "            \n",
    "            # If we get here, task is valid\n",
    "            valid_tasks[task_id] = task_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            invalid_tasks.append((task_id, f\"Validation error: {str(e)}\"))\n",
    "    \n",
    "    print(f\"‚úÖ Valid tasks: {len(valid_tasks)}\")\n",
    "    if invalid_tasks:\n",
    "        print(f\"‚ö†Ô∏è  Invalid tasks: {len(invalid_tasks)}\")\n",
    "        for task_id, reason in invalid_tasks[:5]:  # Show first 5\n",
    "            print(f\"    {task_id}: {reason}\")\n",
    "    \n",
    "    # Task complexity analysis\n",
    "    complexities = []\n",
    "    for task_id, task_data in valid_tasks.items():\n",
    "        complexity = 0\n",
    "        complexity += len(task_data['train'])  # More examples = more complex\n",
    "        complexity += len(task_data['test'])\n",
    "        \n",
    "        # Grid size complexity\n",
    "        for example in task_data['train']:\n",
    "            input_size = np.array(example['input']).size\n",
    "            output_size = np.array(example['output']).size\n",
    "            complexity += (input_size + output_size) / 100\n",
    "        \n",
    "        complexities.append(complexity)\n",
    "    \n",
    "    avg_complexity = np.mean(complexities)\n",
    "    print(f\"üìä Average task complexity: {avg_complexity:.2f}\")\n",
    "    print(f\"üìä Complexity range: {np.min(complexities):.1f} - {np.max(complexities):.1f}\")\n",
    "    \n",
    "    return valid_tasks\n",
    "\n",
    "# Load data\n",
    "test_data = load_and_validate_competition_data()\n",
    "task_names = list(test_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Main Competition Execution\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main_competition_execution():\n",
    "    \"\"\"Main execution function optimized for competition performance.\"\"\"\n",
    "    \n",
    "    print(\"\\nüèÜ Starting Competition Execution\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize ensemble solver\n",
    "    solver = EnsembleArcSolver(config)\n",
    "    \n",
    "    # Execution metrics\n",
    "    total_start_time = time.time()\n",
    "    all_results = {}\n",
    "    execution_stats = {\n",
    "        'successful_tasks': 0,\n",
    "        'failed_tasks': 0,\n",
    "        'total_time': 0,\n",
    "        'avg_confidence': 0,\n",
    "        'avg_ensemble_size': 0,\n",
    "    }\n",
    "    \n",
    "    # Estimate total time\n",
    "    estimated_total_time = len(task_names) * config.SOLVING_CONFIG['time_limit_per_task'] / config.SOLVING_CONFIG['max_workers']\n",
    "    print(f\"üìÖ Estimated completion time: {estimated_total_time/60:.1f} minutes\")\n",
    "    print(f\"üîß Using {config.SOLVING_CONFIG['max_workers']} workers\")\n",
    "    \n",
    "    # Progress tracking\n",
    "    completed_tasks = 0\n",
    "    \n",
    "    try:\n",
    "        # Process tasks in batches for better memory management\n",
    "        batch_size = max(1, config.SOLVING_CONFIG['max_workers'])\n",
    "        \n",
    "        for i in range(0, len(task_names), batch_size):\n",
    "            batch_tasks = task_names[i:i + batch_size]\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            print(f\"\\nüì¶ Processing batch {i//batch_size + 1}/{(len(task_names) + batch_size - 1)//batch_size}\")\n",
    "            print(f\"   Tasks: {len(batch_tasks)} ({batch_tasks[0]} to {batch_tasks[-1]})\")\n",
    "            \n",
    "            # Process batch\n",
    "            if config.SOLVING_CONFIG['max_workers'] == 1:\n",
    "                # Sequential processing\n",
    "                for task_name in batch_tasks:\n",
    "                    task_start = time.time()\n",
    "                    \n",
    "                    try:\n",
    "                        result = solver.solve_task_with_ensemble(\n",
    "                            task_name, \n",
    "                            test_data[task_name], \n",
    "                            config.SOLVING_CONFIG['time_limit_per_task']\n",
    "                        )\n",
    "                        \n",
    "                        all_results[task_name] = result\n",
    "                        execution_stats['successful_tasks'] += 1\n",
    "                        execution_stats['avg_confidence'] += result['confidence']\n",
    "                        execution_stats['avg_ensemble_size'] += result['ensemble_size']\n",
    "                        \n",
    "                        task_time = time.time() - task_start\n",
    "                        completed_tasks += 1\n",
    "                        \n",
    "                        # Progress update\n",
    "                        progress = completed_tasks / len(task_names) * 100\n",
    "                        elapsed = time.time() - total_start_time\n",
    "                        remaining = (elapsed / completed_tasks) * (len(task_names) - completed_tasks)\n",
    "                        \n",
    "                        print(f\"‚úÖ {task_name}: {task_time:.1f}s | \"\n",
    "                              f\"Progress: {progress:.1f}% | \"\n",
    "                              f\"ETA: {remaining/60:.1f}min\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå {task_name} failed: {e}\")\n",
    "                        # Add fallback result\n",
    "                        all_results[task_name] = {\n",
    "                            'solutions': [{'attempt_1': [[0]], 'attempt_2': [[0]]}],\n",
    "                            'confidence': 0.0,\n",
    "                            'ensemble_size': 0,\n",
    "                            'total_time': 0\n",
    "                        }\n",
    "                        execution_stats['failed_tasks'] += 1\n",
    "                        completed_tasks += 1\n",
    "                    \n",
    "                    # Memory cleanup after each task\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "            \n",
    "            else:\n",
    "                # Parallel processing (simplified for notebook)\n",
    "                print(\"‚ö†Ô∏è  Parallel processing not fully implemented in notebook mode\")\n",
    "                # Would implement multiprocessing here for actual deployment\n",
    "                \n",
    "                # For now, fall back to sequential\n",
    "                for task_name in batch_tasks:\n",
    "                    # ... (same sequential code as above)\n",
    "                    pass\n",
    "            \n",
    "            batch_time = time.time() - batch_start_time\n",
    "            print(f\"üì¶ Batch completed in {batch_time:.1f}s\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è  Execution interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Critical error: {e}\")\n",
    "    \n",
    "    # Final statistics\n",
    "    total_execution_time = time.time() - total_start_time\n",
    "    execution_stats['total_time'] = total_execution_time\n",
    "    \n",
    "    if execution_stats['successful_tasks'] > 0:\n",
    "        execution_stats['avg_confidence'] /= execution_stats['successful_tasks']\n",
    "        execution_stats['avg_ensemble_size'] /= execution_stats['successful_tasks']\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üèÅ EXECUTION COMPLETED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚úÖ Successful tasks: {execution_stats['successful_tasks']}/{len(task_names)}\")\n",
    "    print(f\"‚ùå Failed tasks: {execution_stats['failed_tasks']}\")\n",
    "    print(f\"‚è±Ô∏è  Total time: {total_execution_time/60:.1f} minutes\")\n",
    "    print(f\"üìä Success rate: {execution_stats['successful_tasks']/len(task_names)*100:.1f}%\")\n",
    "    print(f\"üéØ Average confidence: {execution_stats['avg_confidence']:.3f}\")\n",
    "    print(f\"ü§ñ Average ensemble size: {execution_stats['avg_ensemble_size']:.1f}\")\n",
    "    \n",
    "    return all_results, execution_stats\n",
    "\n",
    "# Execute main competition pipeline\n",
    "results, stats = main_competition_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Results Processing and Submission Creation\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_competition_submission(results: Dict[str, Any], stats: Dict[str, Any]) -> str:\n",
    "    \"\"\"Create optimized competition submission with quality checks.\"\"\"\n",
    "    \n",
    "    print(\"\\nüìã Creating Competition Submission\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Convert results to submission format\n",
    "    submission = {}\n",
    "    quality_metrics = {\n",
    "        'high_confidence_tasks': 0,\n",
    "        'medium_confidence_tasks': 0,\n",
    "        'low_confidence_tasks': 0,\n",
    "        'default_solutions': 0,\n",
    "    }\n",
    "    \n",
    "    for task_name in task_names:\n",
    "        if task_name in results:\n",
    "            result = results[task_name]\n",
    "            solutions = result.get('solutions', [])\n",
    "            confidence = result.get('confidence', 0.0)\n",
    "            \n",
    "            # Quality classification\n",
    "            if confidence > 0.7:\n",
    "                quality_metrics['high_confidence_tasks'] += 1\n",
    "            elif confidence > 0.3:\n",
    "                quality_metrics['medium_confidence_tasks'] += 1\n",
    "            else:\n",
    "                quality_metrics['low_confidence_tasks'] += 1\n",
    "            \n",
    "            submission[task_name] = solutions\n",
    "        else:\n",
    "            # Default solution for missing tasks\n",
    "            print(f\"‚ö†Ô∏è  Missing result for {task_name}, using default\")\n",
    "            submission[task_name] = [{'attempt_1': [[0]], 'attempt_2': [[0]]}]\n",
    "            quality_metrics['default_solutions'] += 1\n",
    "    \n",
    "    # Validate submission format\n",
    "    validation_errors = []\n",
    "    \n",
    "    for task_name, solutions in submission.items():\n",
    "        if not isinstance(solutions, list):\n",
    "            validation_errors.append(f\"{task_name}: solutions not a list\")\n",
    "            continue\n",
    "        \n",
    "        for i, solution in enumerate(solutions):\n",
    "            if not isinstance(solution, dict):\n",
    "                validation_errors.append(f\"{task_name}[{i}]: solution not a dict\")\n",
    "                continue\n",
    "            \n",
    "            if 'attempt_1' not in solution or 'attempt_2' not in solution:\n",
    "                validation_errors.append(f\"{task_name}[{i}]: missing attempt_1 or attempt_2\")\n",
    "                continue\n",
    "            \n",
    "            # Validate attempts are proper grid format\n",
    "            for attempt_key in ['attempt_1', 'attempt_2']:\n",
    "                attempt = solution[attempt_key]\n",
    "                if not isinstance(attempt, list) or not all(isinstance(row, list) for row in attempt):\n",
    "                    validation_errors.append(f\"{task_name}[{i}][{attempt_key}]: invalid grid format\")\n",
    "    \n",
    "    # Report validation results\n",
    "    if validation_errors:\n",
    "        print(f\"‚ùå Validation errors found: {len(validation_errors)}\")\n",
    "        for error in validation_errors[:5]:  # Show first 5\n",
    "            print(f\"    {error}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Submission format validation passed\")\n",
    "    \n",
    "    # Quality report\n",
    "    print(f\"\\nüìä Submission Quality Report:\")\n",
    "    print(f\"   High confidence (>70%): {quality_metrics['high_confidence_tasks']}\")\n",
    "    print(f\"   Medium confidence (30-70%): {quality_metrics['medium_confidence_tasks']}\")\n",
    "    print(f\"   Low confidence (<30%): {quality_metrics['low_confidence_tasks']}\")\n",
    "    print(f\"   Default solutions: {quality_metrics['default_solutions']}\")\n",
    "    \n",
    "    # Save submission\n",
    "    submission_file = f\"{config.OUTPUT_PATH}/submission.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(submission_file, 'w') as f:\n",
    "            json.dump(submission, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Submission saved to: {submission_file}\")\n",
    "        \n",
    "        # Verify file\n",
    "        file_size = os.path.getsize(submission_file) / (1024 * 1024)  # MB\n",
    "        print(f\"üìÅ File size: {file_size:.2f} MB\")\n",
    "        \n",
    "        # Quick verification\n",
    "        with open(submission_file, 'r') as f:\n",
    "            verify_submission = json.load(f)\n",
    "        \n",
    "        if len(verify_submission) == len(task_names):\n",
    "            print(\"‚úÖ Submission verification passed\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Submission has {len(verify_submission)} tasks, expected {len(task_names)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save submission: {e}\")\n",
    "        \n",
    "        # Fallback save\n",
    "        try:\n",
    "            fallback_file = \"submission_fallback.json\"\n",
    "            with open(fallback_file, 'w') as f:\n",
    "                json.dump(submission, f)\n",
    "            print(f\"üíæ Fallback submission saved to: {fallback_file}\")\n",
    "            submission_file = fallback_file\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Fallback save also failed: {e2}\")\n",
    "            submission_file = None\n",
    "    \n",
    "    return submission_file\n",
    "\n",
    "# Create final submission\n",
    "submission_file = create_competition_submission(results, stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Final Cleanup and Summary\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def final_cleanup_and_summary():\n",
    "    \"\"\"Final cleanup and competition summary.\"\"\"\n",
    "    \n",
    "    print(\"\\nüßπ Final Cleanup\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory before cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    \n",
    "    # CPU cleanup\n",
    "    gc.collect()\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\nüèÜ COMPETITION SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìù Configuration: {config.MODEL_CONFIG['variant']}\")\n",
    "    print(f\"ü§ñ Model: {config.MODEL_CONFIG['n_layers']} layers, {config.MODEL_CONFIG['base_dim']} dim\")\n",
    "    print(f\"‚ö° Features: Attention={config.MODEL_CONFIG['use_attention']}, \"\n",
    "          f\"Adaptive={config.MODEL_CONFIG['use_adaptive_arch']}\")\n",
    "    print(f\"üî¢ Tasks processed: {len(task_names)}\")\n",
    "    print(f\"‚úÖ Success rate: {stats['successful_tasks']/len(task_names)*100:.1f}%\")\n",
    "    print(f\"‚è±Ô∏è  Total time: {stats['total_time']/60:.1f} minutes\")\n",
    "    print(f\"üìÅ Submission: {submission_file}\")\n",
    "    \n",
    "    # Performance analysis\n",
    "    if stats['successful_tasks'] > 0:\n",
    "        avg_time_per_task = stats['total_time'] / len(task_names)\n",
    "        print(f\"‚ö° Average time per task: {avg_time_per_task:.1f}s\")\n",
    "        print(f\"üéØ Average confidence: {stats['avg_confidence']:.3f}\")\n",
    "        print(f\"ü§ñ Average ensemble size: {stats['avg_ensemble_size']:.1f}\")\n",
    "    \n",
    "    print(\"\\nüéâ Competition execution completed successfully!\")\n",
    "    print(\"üì§ Ready for submission to Kaggle leaderboard!\")\n",
    "\n",
    "# Execute final cleanup\n",
    "final_cleanup_and_summary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
