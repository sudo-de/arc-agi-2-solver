{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ARC-AGI 2025 Submission - VAE Decoder Approach\n",
    "\n",
    "# This notebook implements a sophisticated VAE decoder approach for solving ARC-AGI tasks.\n",
    "\n",
    "# ## Architecture Overview\n",
    "# - Multi-tensor system for handling 5D data (examples, colors, directions, x, y)\n",
    "# - VAE decoder with KL divergence loss\n",
    "# - Directional processing layers (cummax, shift)\n",
    "# - Solution selection based on uncertainty scoring\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "import traceback\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add the compressarc dataset path\n",
    "sys.path.append('/kaggle/input/compressarc')\n",
    "\n",
    "print(\"Importing ARC solver modules...\")\n",
    "try:\n",
    "    import multitensor_systems\n",
    "    import preprocessing\n",
    "    import arc_compressor\n",
    "    import initializers\n",
    "    import layers\n",
    "    import solution_selection\n",
    "    import train\n",
    "    import solve_task\n",
    "    print(\"✓ All modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import error: {e}\")\n",
    "    print(\"Make sure the 'compressarc' dataset is added as input\")\n",
    "\n",
    "# Setup environment and check GPU availability\n",
    "print(\"Setting up environment...\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Configure device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_name(0)\n",
    "    memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    torch.set_default_device('cuda')\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "    print(f\"✓ Using CUDA: {device}\")\n",
    "    print(f\"✓ GPU Memory: {memory:.1f} GB\")\n",
    "else:\n",
    "    torch.set_default_device('cpu')\n",
    "    print(\"⚠ Using CPU (CUDA not available)\")\n",
    "\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ NumPy version: {np.__version__}\")\n",
    "\n",
    "# Load and explore test data\n",
    "print(\"Loading test challenges...\")\n",
    "\n",
    "try:\n",
    "    with open('/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json', 'r') as f:\n",
    "        test_challenges = json.load(f)\n",
    "    \n",
    "    print(f\"✓ Found {len(test_challenges)} test tasks\")\n",
    "    \n",
    "    # Analyze task characteristics\n",
    "    task_stats = {\n",
    "        'total_tasks': len(test_challenges),\n",
    "        'train_examples': [],\n",
    "        'test_examples': [],\n",
    "        'grid_sizes': []\n",
    "    }\n",
    "    \n",
    "    for task_name, task_data in test_challenges.items():\n",
    "        task_stats['train_examples'].append(len(task_data['train']))\n",
    "        task_stats['test_examples'].append(len(task_data['test']))\n",
    "        \n",
    "        # Collect grid sizes\n",
    "        for example in task_data['train'] + task_data['test']:\n",
    "            input_shape = np.array(example['input']).shape\n",
    "            task_stats['grid_sizes'].append(input_shape)\n",
    "            if 'output' in example:\n",
    "                output_shape = np.array(example['output']).shape\n",
    "                task_stats['grid_sizes'].append(output_shape)\n",
    "    \n",
    "    print(f\"Train examples per task: {np.mean(task_stats['train_examples']):.1f} ± {np.std(task_stats['train_examples']):.1f}\")\n",
    "    print(f\"Test examples per task: {np.mean(task_stats['test_examples']):.1f} ± {np.std(task_stats['test_examples']):.1f}\")\n",
    "    \n",
    "    grid_sizes = np.array(task_stats['grid_sizes'])\n",
    "    print(f\"Grid sizes - X: {grid_sizes[:, 0].min()}-{grid_sizes[:, 0].max()}, Y: {grid_sizes[:, 1].min()}-{grid_sizes[:, 1].max()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading test data: {e}\")\n",
    "    test_challenges = {}\n",
    "\n",
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    'max_iterations': 800,        # Training iterations per task\n",
    "    'time_limit_per_task': 45,    # Seconds per task\n",
    "    'early_stopping_threshold': 100,  # Steps before checking convergence\n",
    "    'convergence_check_interval': 50,  # Steps between convergence checks\n",
    "    'memory_cleanup_interval': 20,     # Tasks between memory cleanup\n",
    "    'progress_report_interval': 10,    # Tasks between progress reports\n",
    "    'learning_rate': 0.01,\n",
    "    'adam_betas': (0.5, 0.9)\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Define enhanced task solving function with monitoring\n",
    "def solve_task_with_monitoring(task_name, problem_data, config):\n",
    "    \"\"\"\n",
    "    Solve a single ARC task with detailed monitoring and logging.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create task object\n",
    "        task = preprocessing.Task(task_name, problem_data, None)\n",
    "        \n",
    "        # Initialize model and optimizer\n",
    "        model = arc_compressor.ARCCompressor(task)\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.weights_list, \n",
    "            lr=config['learning_rate'], \n",
    "            betas=config['adam_betas']\n",
    "        )\n",
    "        logger = solution_selection.Logger(task)\n",
    "        \n",
    "        # Initialize default solutions\n",
    "        default_solution = tuple(((0, 0), (0, 0)) for _ in range(task.n_test))\n",
    "        logger.solution_most_frequent = default_solution\n",
    "        logger.solution_second_most_frequent = default_solution\n",
    "        \n",
    "        # Training metrics\n",
    "        metrics = {\n",
    "            'steps_completed': 0,\n",
    "            'final_loss': None,\n",
    "            'convergence_step': None,\n",
    "            'memory_used': 0\n",
    "        }\n",
    "        \n",
    "        # Training loop\n",
    "        for step in range(config['max_iterations']):\n",
    "            if time.time() - start_time > config['time_limit_per_task']:\n",
    "                print(f\"  Time limit reached at step {step}\")\n",
    "                break\n",
    "            \n",
    "            # Training step\n",
    "            train.take_step(task, model, optimizer, step, logger)\n",
    "            metrics['steps_completed'] = step + 1\n",
    "            \n",
    "            # Store final loss\n",
    "            if len(logger.loss_curve) > 0:\n",
    "                metrics['final_loss'] = logger.loss_curve[-1]\n",
    "            \n",
    "            # Check for convergence\n",
    "            if (step > config['early_stopping_threshold'] and \n",
    "                step % config['convergence_check_interval'] == 0):\n",
    "                \n",
    "                if (logger.solution_most_frequent is not None and \n",
    "                    logger.solution_second_most_frequent is not None and\n",
    "                    logger.solution_most_frequent != default_solution):\n",
    "                    \n",
    "                    metrics['convergence_step'] = step\n",
    "                    print(f\"  Converged at step {step}\")\n",
    "                    break\n",
    "        \n",
    "        # Extract solutions\n",
    "        solutions = []\n",
    "        for example_num in range(task.n_test):\n",
    "            if (logger.solution_most_frequent is not None and \n",
    "                len(logger.solution_most_frequent) > example_num):\n",
    "                attempt_1 = [list(row) for row in logger.solution_most_frequent[example_num]]\n",
    "            else:\n",
    "                attempt_1 = [[0]]\n",
    "                \n",
    "            if (logger.solution_second_most_frequent is not None and \n",
    "                len(logger.solution_second_most_frequent) > example_num):\n",
    "                attempt_2 = [list(row) for row in logger.solution_second_most_frequent[example_num]]\n",
    "            else:\n",
    "                attempt_2 = [[0]]\n",
    "                \n",
    "            solutions.append({'attempt_1': attempt_1, 'attempt_2': attempt_2})\n",
    "        \n",
    "        # Memory tracking\n",
    "        if torch.cuda.is_available():\n",
    "            metrics['memory_used'] = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "        \n",
    "        # Cleanup\n",
    "        del task, model, optimizer, logger\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        metrics['elapsed_time'] = elapsed\n",
    "        \n",
    "        return solutions, metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        elapsed = time.time() - start_time\n",
    "        metrics = {\n",
    "            'steps_completed': 0,\n",
    "            'final_loss': None,\n",
    "            'convergence_step': None,\n",
    "            'memory_used': 0,\n",
    "            'elapsed_time': elapsed,\n",
    "            'error': str(e)\n",
    "        }\n",
    "        return [{'attempt_1': [[0]], 'attempt_2': [[0]]}], metrics\n",
    "\n",
    "# Main solving loop with comprehensive monitoring\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING ARC-AGI TASK SOLVING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not test_challenges:\n",
    "    print(\"No test challenges loaded. Exiting.\")\n",
    "else:\n",
    "    all_solutions = {}\n",
    "    all_metrics = {}\n",
    "    total_tasks = len(test_challenges)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Progress tracking\n",
    "    successful_tasks = 0\n",
    "    failed_tasks = 0\n",
    "    total_steps = 0\n",
    "    total_memory = 0\n",
    "    \n",
    "    print(f\"Processing {total_tasks} tasks...\\n\")\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(test_challenges.items(), desc=\"Solving tasks\", unit=\"task\")\n",
    "    \n",
    "    for i, (task_name, problem_data) in enumerate(pbar):\n",
    "        pbar.set_description(f\"Solving {task_name}\")\n",
    "        \n",
    "        # Solve task\n",
    "        solutions, metrics = solve_task_with_monitoring(task_name, problem_data, CONFIG)\n",
    "        \n",
    "        # Store results\n",
    "        all_solutions[task_name] = solutions\n",
    "        all_metrics[task_name] = metrics\n",
    "        \n",
    "        # Update statistics\n",
    "        if 'error' in metrics:\n",
    "            failed_tasks += 1\n",
    "        else:\n",
    "            successful_tasks += 1\n",
    "        \n",
    "        total_steps += metrics['steps_completed']\n",
    "        total_memory += metrics['memory_used']\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Success': f\"{successful_tasks}/{i+1}\",\n",
    "            'Avg_time': f\"{metrics['elapsed_time']:.1f}s\",\n",
    "            'Steps': metrics['steps_completed']\n",
    "        })\n",
    "        \n",
    "        # Periodic progress report\n",
    "        if (i + 1) % CONFIG['progress_report_interval'] == 0:\n",
    "            elapsed_total = time.time() - start_time\n",
    "            avg_time = elapsed_total / (i + 1)\n",
    "            estimated_total = avg_time * total_tasks\n",
    "            remaining = estimated_total - elapsed_total\n",
    "            \n",
    "            print(f\"\\nProgress Update [{i+1}/{total_tasks}]:\")\n",
    "            print(f\"  Successful: {successful_tasks}, Failed: {failed_tasks}\")\n",
    "            print(f\"  Avg time per task: {avg_time:.1f}s\")\n",
    "            print(f\"  Estimated remaining: {remaining/60:.1f} minutes\")\n",
    "            print(f\"  Avg steps per task: {total_steps/(i+1):.0f}\")\n",
    "            print(f\"  Avg memory per task: {total_memory/(i+1):.2f} GB\")\n",
    "        \n",
    "        # Memory cleanup\n",
    "        if (i + 1) % CONFIG['memory_cleanup_interval'] == 0:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    pbar.close()\n",
    "    print(\"\\nTask solving completed!\")\n",
    "\n",
    "# Verify solutions and generate final statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SOLUTION VERIFICATION & STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify all tasks have solutions\n",
    "missing_tasks = []\n",
    "for task_name in test_challenges.keys():\n",
    "    if task_name not in all_solutions:\n",
    "        missing_tasks.append(task_name)\n",
    "        print(f\"⚠ Missing solution for {task_name}, adding default\")\n",
    "        all_solutions[task_name] = [{'attempt_1': [[0]], 'attempt_2': [[0]]}]\n",
    "\n",
    "if missing_tasks:\n",
    "    print(f\"\\nAdded default solutions for {len(missing_tasks)} missing tasks\")\n",
    "else:\n",
    "    print(\"✓ All tasks have solutions!\")\n",
    "\n",
    "# Calculate final statistics\n",
    "total_elapsed = time.time() - start_time\n",
    "metrics_values = list(all_metrics.values())\n",
    "\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"  Total tasks: {len(all_solutions)}\")\n",
    "print(f\"  Successful tasks: {successful_tasks}\")\n",
    "print(f\"  Failed tasks: {failed_tasks}\")\n",
    "print(f\"  Success rate: {successful_tasks/len(all_solutions)*100:.1f}%\")\n",
    "print(f\"  Total time: {total_elapsed/60:.1f} minutes\")\n",
    "print(f\"  Average time per task: {total_elapsed/len(all_solutions):.1f} seconds\")\n",
    "\n",
    "if metrics_values:\n",
    "    avg_steps = np.mean([m['steps_completed'] for m in metrics_values])\n",
    "    avg_memory = np.mean([m['memory_used'] for m in metrics_values])\n",
    "    convergence_rate = sum(1 for m in metrics_values if m.get('convergence_step') is not None) / len(metrics_values)\n",
    "    \n",
    "    print(f\"  Average steps per task: {avg_steps:.0f}\")\n",
    "    print(f\"  Average memory per task: {avg_memory:.2f} GB\")\n",
    "    print(f\"  Convergence rate: {convergence_rate*100:.1f}%\")\n",
    "\n",
    "# Analyze solution characteristics\n",
    "solution_sizes = []\n",
    "for task_solutions in all_solutions.values():\n",
    "    for example_solution in task_solutions:\n",
    "        for attempt in ['attempt_1', 'attempt_2']:\n",
    "            grid = example_solution[attempt]\n",
    "            solution_sizes.append((len(grid), len(grid[0]) if grid else 0))\n",
    "\n",
    "solution_sizes = np.array(solution_sizes)\n",
    "print(f\"\\nSolution Grid Sizes:\")\n",
    "print(f\"  X dimension: {solution_sizes[:, 0].min()}-{solution_sizes[:, 0].max()} (avg: {solution_sizes[:, 0].mean():.1f})\")\n",
    "print(f\"  Y dimension: {solution_sizes[:, 1].min()}-{solution_sizes[:, 1].max()} (avg: {solution_sizes[:, 1].mean():.1f})\")\n",
    "\n",
    "# Generate and save submission file\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING SUBMISSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Save submission file\n",
    "    submission_file = 'submission.json'\n",
    "    print(f\"Saving submission to {submission_file}...\")\n",
    "    \n",
    "    with open(submission_file, 'w') as f:\n",
    "        json.dump(all_solutions, f, separators=(',', ':'))  # Compact format\n",
    "    \n",
    "    # Verify submission file\n",
    "    with open(submission_file, 'r') as f:\n",
    "        verification = json.load(f)\n",
    "    \n",
    "    # Validation checks\n",
    "    print(\"\\nSubmission Validation:\")\n",
    "    \n",
    "    if len(verification) == len(test_challenges):\n",
    "        print(f\"✓ Task count: {len(verification)} (matches expected)\")\n",
    "    else:\n",
    "        print(f\"✗ Task count: {len(verification)} (expected {len(test_challenges)})\")\n",
    "    \n",
    "    # Check format\n",
    "    format_valid = True\n",
    "    for task_name, solutions in verification.items():\n",
    "        if not isinstance(solutions, list):\n",
    "            print(f\"✗ Invalid format for {task_name}: not a list\")\n",
    "            format_valid = False\n",
    "            break\n",
    "        \n",
    "        for i, solution in enumerate(solutions):\n",
    "            if not isinstance(solution, dict):\n",
    "                print(f\"✗ Invalid format for {task_name}[{i}]: not a dict\")\n",
    "                format_valid = False\n",
    "                break\n",
    "            \n",
    "            if 'attempt_1' not in solution or 'attempt_2' not in solution:\n",
    "                print(f\"✗ Missing attempts for {task_name}[{i}]\")\n",
    "                format_valid = False\n",
    "                break\n",
    "    \n",
    "    if format_valid:\n",
    "        print(\"✓ Format validation passed\")\n",
    "    \n",
    "    # File size check\n",
    "    file_size = os.path.getsize(submission_file) / (1024 * 1024)  # MB\n",
    "    print(f\"✓ File size: {file_size:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\n🎉 Submission file '{submission_file}' generated successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error generating submission: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Optional: Save detailed metrics for analysis\n",
    "print(\"\\nSaving detailed metrics...\")\n",
    "\n",
    "try:\n",
    "    metrics_file = 'detailed_metrics.json'\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        json.dump(all_metrics, f, indent=2)\n",
    "    print(f\"✓ Metrics saved to {metrics_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not save metrics: {e}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBMISSION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📁 Submission file: submission.json\")\n",
    "print(f\"📊 Tasks solved: {len(all_solutions)}\")\n",
    "print(f\"⏱️ Total time: {total_elapsed/60:.1f} minutes\")\n",
    "print(f\"🎯 Success rate: {successful_tasks/len(all_solutions)*100:.1f}%\")\n",
    "print(\"\\n🚀 Ready for submission to ARC Prize 2025!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
