{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ARC-AGI Model Analysis & Performance Evaluation\n",
    "\n",
    "# This notebook analyzes model performance, training dynamics, and solution quality:\n",
    "# - Training loss curves and convergence analysis\n",
    "# - Solution quality metrics and scoring\n",
    "# - Error analysis and failure modes\n",
    "# - Model architecture impact studies\n",
    "# - Comparison with baseline approaches\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project path\n",
    "sys.path.append('../')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Analysis environment setup complete\")\n",
    "\n",
    "# Load performance data and metrics\n",
    "def load_performance_data():\n",
    "    \"\"\"Load model performance data from various sources\"\"\"\n",
    "    data = {\n",
    "        'metrics': None,\n",
    "        'solutions': None,\n",
    "        'ground_truth': None,\n",
    "        'training_logs': None\n",
    "    }\n",
    "    \n",
    "    # Load detailed metrics if available\n",
    "    try:\n",
    "        with open('../results/detailed_metrics.json', 'r') as f:\n",
    "            data['metrics'] = json.load(f)\n",
    "        print(f\"‚úì Loaded metrics for {len(data['metrics'])} tasks\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö† No detailed metrics found\")\n",
    "    \n",
    "    # Load submission file\n",
    "    try:\n",
    "        with open('../results/submission.json', 'r') as f:\n",
    "            data['solutions'] = json.load(f)\n",
    "        print(f\"‚úì Loaded solutions for {len(data['solutions'])} tasks\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö† No submission file found\")\n",
    "    \n",
    "    # Load ground truth if available\n",
    "    try:\n",
    "        with open('../data/arc-agi_evaluation_solutions.json', 'r') as f:\n",
    "            data['ground_truth'] = json.load(f)\n",
    "        print(f\"‚úì Loaded ground truth for {len(data['ground_truth'])} tasks\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö† No ground truth found\")\n",
    "    \n",
    "    # Load training logs if available\n",
    "    try:\n",
    "        with open('../results/training_logs.json', 'r') as f:\n",
    "            data['training_logs'] = json.load(f)\n",
    "        print(f\"‚úì Loaded training logs\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö† No training logs found\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Create sample data if no real data available\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample performance data for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_tasks = 50\n",
    "    \n",
    "    # Sample metrics\n",
    "    metrics = {}\n",
    "    for i in range(n_tasks):\n",
    "        task_id = f\"task_{i:03d}\"\n",
    "        metrics[task_id] = {\n",
    "            'steps_completed': np.random.randint(100, 1000),\n",
    "            'final_loss': np.random.exponential(2.0),\n",
    "            'convergence_step': np.random.randint(50, 800) if np.random.random() > 0.3 else None,\n",
    "            'memory_used': np.random.uniform(0.5, 4.0),\n",
    "            'elapsed_time': np.random.uniform(10, 60)\n",
    "        }\n",
    "    \n",
    "    # Sample solutions\n",
    "    solutions = {}\n",
    "    for i in range(n_tasks):\n",
    "        task_id = f\"task_{i:03d}\"\n",
    "        solutions[task_id] = [{\n",
    "            'attempt_1': [[np.random.randint(0, 10) for _ in range(3)] for _ in range(3)],\n",
    "            'attempt_2': [[np.random.randint(0, 10) for _ in range(3)] for _ in range(3)]\n",
    "        }]\n",
    "    \n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'solutions': solutions,\n",
    "        'ground_truth': None,\n",
    "        'training_logs': None\n",
    "    }\n",
    "\n",
    "# Load or create data\n",
    "print(\"Loading performance data...\")\n",
    "perf_data = load_performance_data()\n",
    "\n",
    "if perf_data['metrics'] is None and perf_data['solutions'] is None:\n",
    "    print(\"No performance data found. Creating sample data for demonstration.\")\n",
    "    perf_data = create_sample_data()\n",
    "\n",
    "print(\"Data loading complete\")\n",
    "\n",
    "# Analyze training dynamics\n",
    "def analyze_training_metrics(metrics_data):\n",
    "    \"\"\"Analyze training performance metrics\"\"\"\n",
    "    if not metrics_data:\n",
    "        return None\n",
    "    \n",
    "    analysis = {\n",
    "        'total_tasks': len(metrics_data),\n",
    "        'successful_tasks': 0,\n",
    "        'failed_tasks': 0,\n",
    "        'convergence_rate': 0,\n",
    "        'avg_steps': 0,\n",
    "        'avg_time': 0,\n",
    "        'avg_memory': 0,\n",
    "        'loss_distribution': [],\n",
    "        'time_distribution': [],\n",
    "        'convergence_steps': []\n",
    "    }\n",
    "    \n",
    "    steps_list = []\n",
    "    time_list = []\n",
    "    memory_list = []\n",
    "    loss_list = []\n",
    "    convergence_count = 0\n",
    "    \n",
    "    for task_id, metrics in metrics_data.items():\n",
    "        if 'error' in metrics:\n",
    "            analysis['failed_tasks'] += 1\n",
    "        else:\n",
    "            analysis['successful_tasks'] += 1\n",
    "        \n",
    "        steps_list.append(metrics.get('steps_completed', 0))\n",
    "        time_list.append(metrics.get('elapsed_time', 0))\n",
    "        memory_list.append(metrics.get('memory_used', 0))\n",
    "        \n",
    "        if metrics.get('final_loss') is not None:\n",
    "            loss_list.append(metrics['final_loss'])\n",
    "        \n",
    "        if metrics.get('convergence_step') is not None:\n",
    "            convergence_count += 1\n",
    "            analysis['convergence_steps'].append(metrics['convergence_step'])\n",
    "    \n",
    "    analysis['convergence_rate'] = convergence_count / len(metrics_data) if metrics_data else 0\n",
    "    analysis['avg_steps'] = np.mean(steps_list) if steps_list else 0\n",
    "    analysis['avg_time'] = np.mean(time_list) if time_list else 0\n",
    "    analysis['avg_memory'] = np.mean(memory_list) if memory_list else 0\n",
    "    analysis['loss_distribution'] = loss_list\n",
    "    analysis['time_distribution'] = time_list\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "training_analysis = analyze_training_metrics(perf_data['metrics'])\n",
    "\n",
    "if training_analysis:\n",
    "    print(\"üìä TRAINING PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total tasks: {training_analysis['total_tasks']}\")\n",
    "    print(f\"Successful: {training_analysis['successful_tasks']} ({training_analysis['successful_tasks']/training_analysis['total_tasks']*100:.1f}%)\")\n",
    "    print(f\"Failed: {training_analysis['failed_tasks']} ({training_analysis['failed_tasks']/training_analysis['total_tasks']*100:.1f}%)\")\n",
    "    print(f\"Convergence rate: {training_analysis['convergence_rate']*100:.1f}%\")\n",
    "    print(f\"Average steps: {training_analysis['avg_steps']:.0f}\")\n",
    "    print(f\"Average time: {training_analysis['avg_time']:.1f}s\")\n",
    "    print(f\"Average memory: {training_analysis['avg_memory']:.2f}GB\")\n",
    "else:\n",
    "    print(\"No training metrics available for analysis\")\n",
    "\n",
    "# Visualize training performance\n",
    "if training_analysis and training_analysis['total_tasks'] > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Training Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Success/Failure pie chart\n",
    "    success_data = [training_analysis['successful_tasks'], training_analysis['failed_tasks']]\n",
    "    success_labels = ['Successful', 'Failed']\n",
    "    colors = ['lightgreen', 'lightcoral']\n",
    "    axes[0, 0].pie(success_data, labels=success_labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 0].set_title('Task Success Rate')\n",
    "    \n",
    "    # 2. Training steps distribution\n",
    "    if perf_data['metrics']:\n",
    "        steps = [m.get('steps_completed', 0) for m in perf_data['metrics'].values()]\n",
    "        axes[0, 1].hist(steps, bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].set_xlabel('Training Steps')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_title('Training Steps Distribution')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].axvline(np.mean(steps), color='red', linestyle='--', label=f'Mean: {np.mean(steps):.0f}')\n",
    "        axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Loss distribution\n",
    "    if training_analysis['loss_distribution']:\n",
    "        axes[0, 2].hist(training_analysis['loss_distribution'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[0, 2].set_xlabel('Final Loss')\n",
    "        axes[0, 2].set_ylabel('Frequency')\n",
    "        axes[0, 2].set_title('Final Loss Distribution')\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        axes[0, 2].set_yscale('log')\n",
    "    \n",
    "    # 4. Time vs Performance scatter\n",
    "    if perf_data['metrics']:\n",
    "        times = [m.get('elapsed_time', 0) for m in perf_data['metrics'].values()]\n",
    "        steps = [m.get('steps_completed', 0) for m in perf_data['metrics'].values()]\n",
    "        scatter = axes[1, 0].scatter(times, steps, alpha=0.6, c=steps, cmap='viridis')\n",
    "        axes[1, 0].set_xlabel('Elapsed Time (s)')\n",
    "        axes[1, 0].set_ylabel('Training Steps')\n",
    "        axes[1, 0].set_title('Time vs Training Steps')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    \n",
    "    # 5. Memory usage distribution\n",
    "    if perf_data['metrics']:\n",
    "        memory = [m.get('memory_used', 0) for m in perf_data['metrics'].values()]\n",
    "        axes[1, 1].hist(memory, bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 1].set_xlabel('Memory Usage (GB)')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].set_title('Memory Usage Distribution')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].axvline(np.mean(memory), color='red', linestyle='--', label=f'Mean: {np.mean(memory):.2f}GB')\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    # 6. Convergence analysis\n",
    "    if training_analysis['convergence_steps']:\n",
    "        axes[1, 2].hist(training_analysis['convergence_steps'], bins=15, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 2].set_xlabel('Convergence Step')\n",
    "        axes[1, 2].set_ylabel('Frequency')\n",
    "        axes[1, 2].set_title(f'Convergence Steps (Rate: {training_analysis[\"convergence_rate\"]*100:.1f}%)')\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "        if training_analysis['convergence_steps']:\n",
    "            mean_conv = np.mean(training_analysis['convergence_steps'])\n",
    "            axes[1, 2].axvline(mean_conv, color='red', linestyle='--', label=f'Mean: {mean_conv:.0f}')\n",
    "            axes[1, 2].legend()\n",
    "    else:\n",
    "        axes[1, 2].text(0.5, 0.5, 'No convergence\\ndata available', \n",
    "                       ha='center', va='center', transform=axes[1, 2].transAxes)\n",
    "        axes[1, 2].set_title('Convergence Analysis')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient data for training performance visualization\")\n",
    "\n",
    "# Solution quality analysis\n",
    "def analyze_solution_quality(solutions_data, ground_truth_data=None):\n",
    "    \"\"\"Analyze the quality of generated solutions\"\"\"\n",
    "    if not solutions_data:\n",
    "        return None\n",
    "    \n",
    "    analysis = {\n",
    "        'total_tasks': len(solutions_data),\n",
    "        'total_attempts': 0,\n",
    "        'solution_sizes': [],\n",
    "        'color_diversity': [],\n",
    "        'attempt_similarity': [],\n",
    "        'accuracy': None\n",
    "    }\n",
    "    \n",
    "    for task_id, task_solutions in solutions_data.items():\n",
    "        for solution_set in task_solutions:\n",
    "            analysis['total_attempts'] += 2  # attempt_1 and attempt_2\n",
    "            \n",
    "            # Analyze each attempt\n",
    "            for attempt_key in ['attempt_1', 'attempt_2']:\n",
    "                attempt = solution_set[attempt_key]\n",
    "                \n",
    "                # Solution size\n",
    "                if attempt and len(attempt) > 0:\n",
    "                    height = len(attempt)\n",
    "                    width = len(attempt[0]) if attempt[0] else 0\n",
    "                    analysis['solution_sizes'].append((height, width))\n",
    "                    \n",
    "                    # Color diversity\n",
    "                    colors = set()\n",
    "                    for row in attempt:\n",
    "                        colors.update(row)\n",
    "                    analysis['color_diversity'].append(len(colors))\n",
    "            \n",
    "            # Similarity between attempts\n",
    "            if ('attempt_1' in solution_set and 'attempt_2' in solution_set and\n",
    "                solution_set['attempt_1'] and solution_set['attempt_2']):\n",
    "                \n",
    "                try:\n",
    "                    arr1 = np.array(solution_set['attempt_1'])\n",
    "                    arr2 = np.array(solution_set['attempt_2'])\n",
    "                    \n",
    "                    if arr1.shape == arr2.shape:\n",
    "                        similarity = np.mean(arr1 == arr2)\n",
    "                        analysis['attempt_similarity'].append(similarity)\n",
    "                except:\n",
    "                    pass  # Skip if arrays can't be compared\n",
    "    \n",
    "    # Calculate accuracy if ground truth is available\n",
    "    if ground_truth_data:\n",
    "        correct_tasks = 0\n",
    "        comparable_tasks = 0\n",
    "        \n",
    "        for task_id in solutions_data.keys():\n",
    "            if task_id in ground_truth_data:\n",
    "                comparable_tasks += 1\n",
    "                \n",
    "                # Check if either attempt matches ground truth\n",
    "                ground_truth = ground_truth_data[task_id]\n",
    "                solutions = solutions_data[task_id]\n",
    "                \n",
    "                for solution_set in solutions:\n",
    "                    for attempt_key in ['attempt_1', 'attempt_2']:\n",
    "                        try:\n",
    "                            attempt = solution_set[attempt_key]\n",
    "                            if attempt == ground_truth[0]:  # Assuming single test case\n",
    "                                correct_tasks += 1\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "        \n",
    "        if comparable_tasks > 0:\n",
    "            analysis['accuracy'] = correct_tasks / comparable_tasks\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "solution_analysis = analyze_solution_quality(perf_data['solutions'], perf_data['ground_truth'])\n",
    "\n",
    "if solution_analysis:\n",
    "    print(\"\\nüéØ SOLUTION QUALITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total tasks: {solution_analysis['total_tasks']}\")\n",
    "    print(f\"Total attempts: {solution_analysis['total_attempts']}\")\n",
    "    \n",
    "    if solution_analysis['solution_sizes']:\n",
    "        sizes = np.array(solution_analysis['solution_sizes'])\n",
    "        print(f\"Average solution size: {np.mean(sizes[:, 0]):.1f} x {np.mean(sizes[:, 1]):.1f}\")\n",
    "        print(f\"Size range: {np.min(sizes[:, 0])}-{np.max(sizes[:, 0])} x {np.min(sizes[:, 1])}-{np.max(sizes[:, 1])}\")\n",
    "    \n",
    "    if solution_analysis['color_diversity']:\n",
    "        print(f\"Average colors per solution: {np.mean(solution_analysis['color_diversity']):.1f}\")\n",
    "    \n",
    "    if solution_analysis['attempt_similarity']:\n",
    "        print(f\"Average similarity between attempts: {np.mean(solution_analysis['attempt_similarity']):.3f}\")\n",
    "    \n",
    "    if solution_analysis['accuracy'] is not None:\n",
    "        print(f\"Accuracy: {solution_analysis['accuracy']*100:.1f}%\")\n",
    "    else:\n",
    "        print(\"Accuracy: Cannot be calculated (no ground truth)\")\n",
    "else:\n",
    "    print(\"No solution data available for analysis\")\n",
    "\n",
    "# Visualize solution characteristics\n",
    "if solution_analysis and solution_analysis['solution_sizes']:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Solution Quality Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Solution size distribution\n",
    "    sizes = np.array(solution_analysis['solution_sizes'])\n",
    "    axes[0, 0].scatter(sizes[:, 1], sizes[:, 0], alpha=0.6)\n",
    "    axes[0, 0].set_xlabel('Width')\n",
    "    axes[0, 0].set_ylabel('Height')\n",
    "    axes[0, 0].set_title('Solution Size Distribution')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Color diversity distribution\n",
    "    if solution_analysis['color_diversity']:\n",
    "        axes[0, 1].hist(solution_analysis['color_diversity'], bins=range(1, 11), \n",
    "                       alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].set_xlabel('Number of Unique Colors')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_title('Color Diversity in Solutions')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Attempt similarity distribution\n",
    "    if solution_analysis['attempt_similarity']:\n",
    "        axes[1, 0].hist(solution_analysis['attempt_similarity'], bins=20, \n",
    "                       alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].set_xlabel('Similarity Score')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title('Similarity Between Attempt 1 & 2')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        mean_sim = np.mean(solution_analysis['attempt_similarity'])\n",
    "        axes[1, 0].axvline(mean_sim, color='red', linestyle='--', \n",
    "                          label=f'Mean: {mean_sim:.3f}')\n",
    "        axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Solution area vs color diversity\n",
    "    if solution_analysis['color_diversity']:\n",
    "        areas = sizes[:, 0] * sizes[:, 1]\n",
    "        # Ensure we have matching lengths\n",
    "        min_len = min(len(areas), len(solution_analysis['color_diversity']))\n",
    "        axes[1, 1].scatter(areas[:min_len], solution_analysis['color_diversity'][:min_len], \n",
    "                          alpha=0.6)\n",
    "        axes[1, 1].set_xlabel('Solution Area (pixels)')\n",
    "        axes[1, 1].set_ylabel('Number of Colors')\n",
    "        axes[1, 1].set_title('Area vs Color Diversity')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient solution data for visualization\")\n",
    "\n",
    "# Error analysis and failure modes\n",
    "def analyze_failure_modes(metrics_data, solutions_data):\n",
    "    \"\"\"Analyze common failure patterns\"\"\"\n",
    "    failure_analysis = {\n",
    "        'timeout_failures': 0,\n",
    "        'convergence_failures': 0,\n",
    "        'error_failures': 0,\n",
    "        'low_quality_solutions': 0,\n",
    "        'failed_task_characteristics': {\n",
    "            'avg_complexity': 0,\n",
    "            'common_patterns': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if not metrics_data:\n",
    "        return failure_analysis\n",
    "    \n",
    "    for task_id, metrics in metrics_data.items():\n",
    "        # Categorize failures\n",
    "        if 'error' in metrics:\n",
    "            failure_analysis['error_failures'] += 1\n",
    "        elif metrics.get('convergence_step') is None:\n",
    "            if metrics.get('steps_completed', 0) >= 700:  # Near max iterations\n",
    "                failure_analysis['timeout_failures'] += 1\n",
    "            else:\n",
    "                failure_analysis['convergence_failures'] += 1\n",
    "    \n",
    "    # Analyze solution quality issues\n",
    "    if solutions_data:\n",
    "        for task_id, task_solutions in solutions_data.items():\n",
    "            for solution_set in task_solutions:\n",
    "                # Check for low quality solutions (e.g., all zeros, very small, etc.)\n",
    "                for attempt_key in ['attempt_1', 'attempt_2']:\n",
    "                    attempt = solution_set[attempt_key]\n",
    "                    if attempt:\n",
    "                        # Check if solution is just zeros or very simple\n",
    "                        flat_solution = [cell for row in attempt for cell in row]\n",
    "                        if all(cell == 0 for cell in flat_solution) or len(set(flat_solution)) == 1:\n",
    "                            failure_analysis['low_quality_solutions'] += 1\n",
    "                            break\n",
    "    \n",
    "    return failure_analysis\n",
    "\n",
    "failure_analysis = analyze_failure_modes(perf_data['metrics'], perf_data['solutions'])\n",
    "\n",
    "print(\"\\nüö® FAILURE MODE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Timeout failures: {failure_analysis['timeout_failures']}\")\n",
    "print(f\"Convergence failures: {failure_analysis['convergence_failures']}\")\n",
    "print(f\"Error failures: {failure_analysis['error_failures']}\")\n",
    "print(f\"Low quality solutions: {failure_analysis['low_quality_solutions']}\")\n",
    "\n",
    "# Visualize failure modes\n",
    "if any(failure_analysis[key] > 0 for key in ['timeout_failures', 'convergence_failures', 'error_failures', 'low_quality_solutions']):\n",
    "    failure_types = ['Timeout', 'Convergence', 'Error', 'Low Quality']\n",
    "    failure_counts = [\n",
    "        failure_analysis['timeout_failures'],\n",
    "        failure_analysis['convergence_failures'], \n",
    "        failure_analysis['error_failures'],\n",
    "        failure_analysis['low_quality_solutions']\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(failure_types, failure_counts, color=['orange', 'red', 'darkred', 'gray'], alpha=0.7)\n",
    "    plt.xlabel('Failure Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Failure Modes')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, failure_counts):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{count}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No significant failure modes detected\")\n",
    "\n",
    "# Performance correlation analysis\n",
    "def correlation_analysis(metrics_data):\n",
    "    \"\"\"Analyze correlations between different performance metrics\"\"\"\n",
    "    if not metrics_data:\n",
    "        return None\n",
    "    \n",
    "    # Extract numeric metrics\n",
    "    data_for_correlation = []\n",
    "    \n",
    "    for task_id, metrics in metrics_data.items():\n",
    "        if 'error' not in metrics:  # Only successful tasks\n",
    "            row = {\n",
    "                'steps_completed': metrics.get('steps_completed', 0),\n",
    "                'elapsed_time': metrics.get('elapsed_time', 0),\n",
    "                'memory_used': metrics.get('memory_used', 0),\n",
    "                'final_loss': metrics.get('final_loss', np.nan),\n",
    "                'converged': 1 if metrics.get('convergence_step') is not None else 0,\n",
    "                'convergence_step': metrics.get('convergence_step', np.nan)\n",
    "            }\n",
    "            data_for_correlation.append(row)\n",
    "    \n",
    "    if len(data_for_correlation) < 2:\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(data_for_correlation)\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "    \n",
    "    return df, correlation_matrix\n",
    "\n",
    "correlation_result = correlation_analysis(perf_data['metrics'])\n",
    "\n",
    "if correlation_result:\n",
    "    df, corr_matrix = correlation_result\n",
    "    \n",
    "    print(\"\\nüìà PERFORMANCE CORRELATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Display key correlations\n",
    "    print(\"Key correlations:\")\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            col1, col2 = corr_matrix.columns[i], corr_matrix.columns[j]\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if not np.isnan(corr_val) and abs(corr_val) > 0.3:\n",
    "                print(f\"{col1} vs {col2}: {corr_val:.3f}\")\n",
    "    \n",
    "    # Visualize correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.2f', cbar_kws={\"shrink\": .5})\n",
    "    plt.title('Performance Metrics Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(\"\\nStatistical Summary:\")\n",
    "    print(df.describe())\n",
    "else:\n",
    "    print(\"Insufficient data for correlation analysis\")\n",
    "\n",
    "# Model architecture impact analysis\n",
    "def architecture_impact_analysis():\n",
    "    \"\"\"Analyze the impact of different architectural choices\"\"\"\n",
    "    \n",
    "    print(\"\\nüèóÔ∏è ARCHITECTURE IMPACT ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Theoretical analysis based on architecture design\n",
    "    architecture_components = {\n",
    "        'Multi-tensor System': {\n",
    "            'purpose': 'Handle variable dimensionality efficiently',\n",
    "            'impact': 'Enables processing of different grid sizes and patterns',\n",
    "            'complexity': 'High'\n",
    "        },\n",
    "        'VAE Decoder': {\n",
    "            'purpose': 'Learn compressed representations with KL regularization',\n",
    "            'impact': 'Encourages meaningful latent representations',\n",
    "            'complexity': 'Medium'\n",
    "        },\n",
    "        'Directional Layers': {\n",
    "            'purpose': 'Spatial reasoning with cummax and shift operations',\n",
    "            'impact': 'Captures spatial patterns and transformations',\n",
    "            'complexity': 'High'\n",
    "        },\n",
    "        'Residual Connections': {\n",
    "            'purpose': 'Enable deep networks and gradient flow',\n",
    "            'impact': 'Improves training stability and convergence',\n",
    "            'complexity': 'Low'\n",
    "        },\n",
    "        'Symmetrization': {\n",
    "            'purpose': 'Ensure equivariance to x/y dimension swapping',\n",
    "            'impact': 'Reduces overfitting to specific orientations',\n",
    "            'complexity': 'Medium'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Display analysis\n",
    "    for component, details in architecture_components.items():\n",
    "        print(f\"\\n{component}:\")\n",
    "        print(f\"  Purpose: {details['purpose']}\")\n",
    "        print(f\"  Impact: {details['impact']}\")\n",
    "        print(f\"  Complexity: {details['complexity']}\")\n",
    "    \n",
    "    # Visualize architecture complexity vs impact\n",
    "    components = list(architecture_components.keys())\n",
    "    complexity_scores = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "    complexities = [complexity_scores[architecture_components[comp]['complexity']] for comp in components]\n",
    "    \n",
    "    # Estimated impact scores (subjective)\n",
    "    impact_scores = [3, 2, 3, 2, 2]  # High impact for multi-tensor and directional layers\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['red' if c == 3 else 'orange' if c == 2 else 'green' for c in complexities]\n",
    "    scatter = plt.scatter(complexities, impact_scores, c=colors, s=100, alpha=0.7)\n",
    "    \n",
    "    for i, comp in enumerate(components):\n",
    "        plt.annotate(comp, (complexities[i], impact_scores[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    plt.xlabel('Implementation Complexity')\n",
    "    plt.ylabel('Estimated Impact')\n",
    "    plt.title('Architecture Components: Complexity vs Impact')\n",
    "    plt.xticks([1, 2, 3], ['Low', 'Medium', 'High'])\n",
    "    plt.yticks([1, 2, 3], ['Low', 'Medium', 'High'])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "architecture_impact_analysis()\n",
    "\n",
    "# Generate comprehensive analysis report\n",
    "def generate_analysis_report(training_analysis, solution_analysis, failure_analysis):\n",
    "    \"\"\"Generate a comprehensive analysis report\"\"\"\n",
    "    \n",
    "    report = f\"\"\"\n",
    "ARC-AGI Model Performance Analysis Report\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "================================================\n",
    "\n",
    "üéØ EXECUTIVE SUMMARY:\n",
    "\"\"\"\n",
    "    \n",
    "    if training_analysis:\n",
    "        success_rate = training_analysis['successful_tasks'] / training_analysis['total_tasks'] * 100\n",
    "        report += f\"\"\"\n",
    "‚Ä¢ Tasks processed: {training_analysis['total_tasks']}\n",
    "‚Ä¢ Success rate: {success_rate:.1f}%\n",
    "‚Ä¢ Average training time: {training_analysis['avg_time']:.1f}s\n",
    "‚Ä¢ Convergence rate: {training_analysis['convergence_rate']*100:.1f}%\n",
    "\"\"\"\n",
    "    \n",
    "    if solution_analysis and solution_analysis['accuracy'] is not None:\n",
    "        report += f\"‚Ä¢ Solution accuracy: {solution_analysis['accuracy']*100:.1f}%\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "üìä DETAILED PERFORMANCE METRICS:\n",
    "\"\"\"\n",
    "    \n",
    "    if training_analysis:\n",
    "        report += f\"\"\"\n",
    "Training Performance:\n",
    "‚Ä¢ Average steps per task: {training_analysis['avg_steps']:.0f}\n",
    "‚Ä¢ Average memory usage: {training_analysis['avg_memory']:.2f} GB\n",
    "‚Ä¢ Failed tasks: {training_analysis['failed_tasks']} ({training_analysis['failed_tasks']/training_analysis['total_tasks']*100:.1f}%)\n",
    "\"\"\"\n",
    "    \n",
    "    if solution_analysis:\n",
    "        report += f\"\"\"\n",
    "Solution Quality:\n",
    "‚Ä¢ Total solution attempts: {solution_analysis['total_attempts']}\n",
    "\"\"\"\n",
    "        if solution_analysis['solution_sizes']:\n",
    "            sizes = np.array(solution_analysis['solution_sizes'])\n",
    "            report += f\"‚Ä¢ Average solution size: {np.mean(sizes[:, 0]):.1f} x {np.mean(sizes[:, 1]):.1f}\\n\"\n",
    "        \n",
    "        if solution_analysis['color_diversity']:\n",
    "            report += f\"‚Ä¢ Average colors per solution: {np.mean(solution_analysis['color_diversity']):.1f}\\n\"\n",
    "        \n",
    "        if solution_analysis['attempt_similarity']:\n",
    "            report += f\"‚Ä¢ Average similarity between attempts: {np.mean(solution_analysis['attempt_similarity']):.3f}\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "üö® FAILURE ANALYSIS:\n",
    "‚Ä¢ Timeout failures: {failure_analysis['timeout_failures']}\n",
    "‚Ä¢ Convergence failures: {failure_analysis['convergence_failures']}\n",
    "‚Ä¢ Error failures: {failure_analysis['error_failures']}\n",
    "‚Ä¢ Low quality solutions: {failure_analysis['low_quality_solutions']}\n",
    "\n",
    "üí° KEY INSIGHTS:\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate insights based on data\n",
    "    insights = []\n",
    "    \n",
    "    if training_analysis and training_analysis['convergence_rate'] > 0.7:\n",
    "        insights.append(\"High convergence rate indicates effective optimization\")\n",
    "    elif training_analysis and training_analysis['convergence_rate'] < 0.3:\n",
    "        insights.append(\"Low convergence rate suggests need for hyperparameter tuning\")\n",
    "    \n",
    "    if solution_analysis and solution_analysis['attempt_similarity']:\n",
    "        avg_sim = np.mean(solution_analysis['attempt_similarity'])\n",
    "        if avg_sim > 0.8:\n",
    "            insights.append(\"High similarity between attempts indicates low diversity\")\n",
    "        elif avg_sim < 0.3:\n",
    "            insights.append(\"Low similarity between attempts shows good exploration\")\n",
    "    \n",
    "    if failure_analysis['timeout_failures'] > failure_analysis['convergence_failures']:\n",
    "        insights.append(\"Many timeout failures suggest increasing iteration limits\")\n",
    "    \n",
    "    if failure_analysis['low_quality_solutions'] > 0:\n",
    "        insights.append(\"Low quality solutions indicate need for better regularization\")\n",
    "    \n",
    "    for insight in insights:\n",
    "        report += f\"‚Ä¢ {insight}\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "üîß RECOMMENDATIONS:\n",
    "‚Ä¢ Consider early stopping mechanisms to reduce timeout failures\n",
    "‚Ä¢ Implement solution quality filtering to improve output\n",
    "‚Ä¢ Monitor memory usage for large-scale deployments\n",
    "‚Ä¢ Analyze failed tasks for common patterns\n",
    "‚Ä¢ Consider ensemble methods to improve accuracy\n",
    "\n",
    "üìà ARCHITECTURE STRENGTHS:\n",
    "‚Ä¢ Multi-tensor system handles variable grid sizes effectively\n",
    "‚Ä¢ VAE approach provides principled uncertainty quantification\n",
    "‚Ä¢ Directional operations capture spatial reasoning patterns\n",
    "‚Ä¢ Residual connections improve training stability\n",
    "\n",
    "‚ö†Ô∏è POTENTIAL IMPROVEMENTS:\n",
    "‚Ä¢ Adaptive training schedules based on task complexity\n",
    "‚Ä¢ Better initialization strategies for faster convergence\n",
    "‚Ä¢ Task-specific architecture modifications\n",
    "‚Ä¢ Enhanced solution selection criteria\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and display report\n",
    "analysis_report = generate_analysis_report(training_analysis, solution_analysis, failure_analysis)\n",
    "print(analysis_report)\n",
    "\n",
    "# Save report to file\n",
    "try:\n",
    "    os.makedirs('../reports', exist_ok=True)\n",
    "    with open('../reports/performance_analysis_report.txt', 'w') as f:\n",
    "        f.write(analysis_report)\n",
    "    print(\"\\nüíæ Analysis report saved to ../reports/performance_analysis_report.txt\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö† Could not save report: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Performance analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
